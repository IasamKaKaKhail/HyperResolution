{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cdc68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129b8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as ks\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import seaborn as sns\n",
    "import scipy as sci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import cv2\n",
    "from math import log10, sqrt\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from keras.utils import to_categorical\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "import mtcnn\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from numpy.linalg import norm\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, concatenate, MaxPooling2D, Dropout, UpSampling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8d7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa45cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['balloons_ms', 'beads_ms', 'cd_ms', 'chart_and_stuffed_toy_ms', 'clay_ms', 'cloth_ms', 'egyptian_statue_ms', 'face_ms', 'fake_and_real_beers_ms', 'fake_and_real_food_ms', 'fake_and_real_lemon_slices_ms', 'fake_and_real_lemons_ms', 'fake_and_real_peppers_ms', 'fake_and_real_strawberries_ms', 'fake_and_real_sushi_ms', 'fake_and_real_tomatoes_ms', 'feathers_ms', 'flowers_ms', 'glass_tiles_ms', 'hairs_ms', 'jelly_beans_ms', 'oil_painting_ms', 'paints_ms', 'photo_and_face_ms', 'pompoms_ms', 'real_and_fake_apples_ms', 'real_and_fake_peppers_ms', 'sponges_ms', 'stuffed_toys_ms', 'superballs_ms', 'thread_spools_ms', 'watercolors_ms']\n",
      "\n",
      " ./dataset/balloons_ms/balloons_ms/\n"
     ]
    }
   ],
   "source": [
    "datasetpath = \"./dataset/\"\n",
    "image_size = 512\n",
    "patch_size = 64\n",
    "stride = 32\n",
    "\n",
    "# Prepare the list to store the ground truth hyperspectral images\n",
    "groundtruth_images = []\n",
    "\n",
    "classes = sorted(os.listdir(datasetpath))\n",
    "print(\"\\n\", classes)\n",
    "\n",
    "dataset_path = datasetpath + classes[0] + \"/\" + classes[0] + \"/\"\n",
    "\n",
    "print(\"\\n\", dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9911cf54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "./dataset/balloons_ms/balloons_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (225, 64, 64, 31)\n",
      "1\n",
      "./dataset/beads_ms/beads_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (450, 64, 64, 31)\n",
      "2\n",
      "./dataset/cd_ms/cd_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (675, 64, 64, 31)\n",
      "3\n",
      "./dataset/chart_and_stuffed_toy_ms/chart_and_stuffed_toy_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (900, 64, 64, 31)\n",
      "4\n",
      "./dataset/clay_ms/clay_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (1125, 64, 64, 31)\n",
      "5\n",
      "./dataset/cloth_ms/cloth_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (1350, 64, 64, 31)\n",
      "6\n",
      "./dataset/egyptian_statue_ms/egyptian_statue_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (1575, 64, 64, 31)\n",
      "7\n",
      "./dataset/face_ms/face_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (1800, 64, 64, 31)\n",
      "8\n",
      "./dataset/fake_and_real_beers_ms/fake_and_real_beers_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (2025, 64, 64, 31)\n",
      "9\n",
      "./dataset/fake_and_real_food_ms/fake_and_real_food_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (2250, 64, 64, 31)\n",
      "10\n",
      "./dataset/fake_and_real_lemon_slices_ms/fake_and_real_lemon_slices_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (2475, 64, 64, 31)\n",
      "11\n",
      "./dataset/fake_and_real_lemons_ms/fake_and_real_lemons_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (2700, 64, 64, 31)\n",
      "12\n",
      "./dataset/fake_and_real_peppers_ms/fake_and_real_peppers_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (2925, 64, 64, 31)\n",
      "13\n",
      "./dataset/fake_and_real_strawberries_ms/fake_and_real_strawberries_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (3150, 64, 64, 31)\n",
      "14\n",
      "./dataset/fake_and_real_sushi_ms/fake_and_real_sushi_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (3375, 64, 64, 31)\n",
      "15\n",
      "./dataset/fake_and_real_tomatoes_ms/fake_and_real_tomatoes_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (3600, 64, 64, 31)\n",
      "16\n",
      "./dataset/feathers_ms/feathers_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (3825, 64, 64, 31)\n",
      "17\n",
      "./dataset/flowers_ms/flowers_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (4050, 64, 64, 31)\n",
      "18\n",
      "./dataset/glass_tiles_ms/glass_tiles_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (4275, 64, 64, 31)\n",
      "19\n",
      "./dataset/hairs_ms/hairs_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (4500, 64, 64, 31)\n",
      "20\n",
      "./dataset/jelly_beans_ms/jelly_beans_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (4725, 64, 64, 31)\n",
      "21\n",
      "./dataset/oil_painting_ms/oil_painting_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (4950, 64, 64, 31)\n",
      "22\n",
      "./dataset/paints_ms/paints_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (5175, 64, 64, 31)\n",
      "23\n",
      "./dataset/photo_and_face_ms/photo_and_face_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (5400, 64, 64, 31)\n",
      "24\n",
      "./dataset/pompoms_ms/pompoms_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (5625, 64, 64, 31)\n",
      "25\n",
      "./dataset/real_and_fake_apples_ms/real_and_fake_apples_ms/\n",
      "Ground Truth Hyperspectral Images Shape: (5850, 64, 64, 31)\n",
      "26\n",
      "./dataset/real_and_fake_peppers_ms/real_and_fake_peppers_ms/\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 31):\n",
    "    print(i)\n",
    "    dataset_path = datasetpath + classes[i] + \"/\" + classes[i] + \"/\"\n",
    "    print(dataset_path)\n",
    "    \n",
    "   # Iterate over the images in the dataset directory\n",
    "    for filename in sorted(os.listdir(dataset_path)):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(dataset_path, filename)\n",
    "            image = imread(image_path)\n",
    "            # Crop the hyperspectral image into smaller patches\n",
    "            for i in range(0, image_size - patch_size + 1, stride):\n",
    "                for j in range(0, image_size - patch_size + 1, stride):\n",
    "                    patch = image[i : i + patch_size, j : j + patch_size]\n",
    "                    groundtruth_images.append(patch)\n",
    "    \n",
    "    # Convert the list of ground truth patches to a numpy array\n",
    "    groundtruth_arrays = np.array(groundtruth_images)\n",
    "\n",
    "    # Reshape the ground truth images to match the specified dimensions\n",
    "    groundtruth_arrays = groundtruth_arrays.reshape(-1, patch_size, patch_size, 31)\n",
    "\n",
    "    # Print the shape of the ground truth hyperspectral images array\n",
    "    print(\"Ground Truth Hyperspectral Images Shape:\", groundtruth_arrays.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5357c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter\n",
    "# Define the size of the averaging filter\n",
    "filter_size = (8, 8)\n",
    "\n",
    "# Apply the averaging filter on each band of the ground truth hyperspectral images\n",
    "lowres_hsi_images = np.zeros((groundtruth_arrays.shape[0], 8, 8, 31))\n",
    "for i in range(groundtruth_arrays.shape[0]):\n",
    "    for j in range(groundtruth_arrays.shape[3]):\n",
    "        band = groundtruth_arrays[i, :, :, j]\n",
    "        lowres_band = uniform_filter(band, size=filter_size)\n",
    "        lowres_hsi_images[i, :, :, j] = lowres_band[::8, ::8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0967f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-Resolution HSI Images Shape: (6300, 8, 8, 31)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the low-resolution HSI images array\n",
    "print(\"Low-Resolution HSI Images Shape:\", lowres_hsi_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 1x1 filters to generate the high-resolution RGB images\n",
    "highres_rgb_images = np.zeros((groundtruth_arrays.shape[0], 64, 64, 3))\n",
    "for i in range(groundtruth_arrays.shape[0]):\n",
    "    for j in range(3):\n",
    "        start_band = j * 10\n",
    "        end_band = (j + 1) * 10\n",
    "        rgb_band_avg = np.mean(groundtruth_arrays[i, :, :, start_band:end_band], axis=-1)\n",
    "        highres_rgb_images[i, :, :, j] = rgb_band_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Resolution RGB Images Shape: (6300, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the high-resolution RGB images array\n",
    "print(\"High-Resolution RGB Images Shape:\", highres_rgb_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_ratio = 0.8\n",
    "num_samples = groundtruth_arrays.shape[0]\n",
    "num_train_samples = int(train_ratio * num_samples)\n",
    "y_train = groundtruth_arrays[:num_train_samples]\n",
    "train_lowres_hsi = lowres_hsi_images[:num_train_samples]\n",
    "train_highres_rgb = highres_rgb_images[:num_train_samples]\n",
    "test_lowres_hsi = lowres_hsi_images[num_train_samples:]\n",
    "test_highres_rgb = highres_rgb_images[num_train_samples:]\n",
    "y_test = groundtruth_arrays[num_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f95ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data (optional but recommended)\n",
    "train_lowres_hsi = train_lowres_hsi / 255.0\n",
    "train_highres_rgb = train_highres_rgb / 255.0\n",
    "test_lowres_hsi = test_lowres_hsi / 255.0\n",
    "test_highres_rgb = test_highres_rgb / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0d20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8, 8, 31)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 8, 8, 64)     17920       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 8, 8, 64)     36928       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 64)   1792        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 1, 64)     0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 64, 64, 64)   36928       ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1, 1, 64)     0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 32, 32, 64)   0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32, 32, 64)   0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 128)  0           ['up_sampling2d[0][0]',          \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 128)  147584      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 128)  147584      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 128)  0          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 64, 31)   35743       ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 424,479\n",
      "Trainable params: 424,479\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the input shapes\n",
    "msi_input_shape = (8, 8, 31)\n",
    "hsi_input_shape = (64, 64, 3)\n",
    "\n",
    "# Define the MSI branch\n",
    "msi_input = Input(shape=msi_input_shape)\n",
    "msi_conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(msi_input)\n",
    "msi_conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(msi_conv1)\n",
    "msi_pooling = MaxPooling2D((8, 8))(msi_conv2)  # Adjust the pooling size\n",
    "msi_dropout = Dropout(0.2)(msi_pooling)\n",
    "\n",
    "# Define the HSI branch\n",
    "hsi_input = Input(shape=hsi_input_shape)\n",
    "hsi_conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(hsi_input)\n",
    "hsi_conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(hsi_conv1)\n",
    "hsi_pooling = MaxPooling2D((2, 2))(hsi_conv2)\n",
    "hsi_dropout = Dropout(0.2)(hsi_pooling)\n",
    "\n",
    "# Upsample MSI features to match HSI features\n",
    "msi_upsampled = UpSampling2D((32, 32))(msi_dropout)  # Adjust the upsampling size\n",
    "\n",
    "# Concatenate the outputs of both branches\n",
    "fusion = concatenate([msi_upsampled, hsi_dropout], axis=-1)\n",
    "\n",
    "# Add more layers for fusion\n",
    "fusion_conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(fusion)\n",
    "fusion_conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(fusion_conv1)\n",
    "fusion_upsampling = UpSampling2D((2, 2))(fusion_conv2)\n",
    "fusion_output = Conv2D(31, (3, 3), activation='relu', padding='same')(fusion_upsampling)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[msi_input, hsi_input], outputs=fusion_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4391219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "126/126 [==============================] - 241s 2s/step - loss: 42872932.0000 - mae: 3611.0830 - val_loss: 50494928.0000 - val_mae: 4208.1172\n",
      "Epoch 2/10\n",
      "126/126 [==============================] - 237s 2s/step - loss: 31931624.0000 - mae: 2976.8145 - val_loss: 50120464.0000 - val_mae: 4211.1074\n",
      "Epoch 3/10\n",
      "126/126 [==============================] - 1261s 10s/step - loss: 31414804.0000 - mae: 2941.9419 - val_loss: 49758796.0000 - val_mae: 4165.8525\n",
      "Epoch 4/10\n",
      "116/126 [==========================>...] - ETA: 15s - loss: 29016504.0000 - mae: 2827.7131"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the necessary parameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "validation_split = 0.2\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x=[train_lowres_hsi, train_highres_rgb],\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 10s 235ms/step - loss: 43015348.0000 - mae: 3804.0876\n",
      "41/41 [==============================] - 8s 184ms/step\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_41_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[1305,64,64,31] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mevaluate(x\u001b[39m=\u001b[39m[test_lowres_hsi, test_highres_rgb], y\u001b[39m=\u001b[39my_test)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Predict on the test set\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict([test_lowres_hsi, test_highres_rgb])\n\u001b[0;32m     10\u001b[0m \u001b[39m# Print the shape of the predicted array\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicted Array Shape:\u001b[39m\u001b[39m\"\u001b[39m, y_pred\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\92305\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\92305\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7214\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7215\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__ConcatV2_N_41_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[1305,64,64,31] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "#compile the model with the optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(x=[test_lowres_hsi, test_highres_rgb], y=y_test)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict([test_lowres_hsi, test_highres_rgb])\n",
    "\n",
    "# Print the shape of the predicted array\n",
    "print(\"Predicted Array Shape:\", y_pred.shape)\n",
    "\n",
    "# Plot the first sample in the test set\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(y_pred[0])\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad118f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/41 [====>.........................] - ETA: 6s"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#plot the results\n",
    "#debug this code above there are errors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Get the model predictions\n",
    "predictions = model.predict([test_lowres_hsi, test_highres_rgb])  # Replace with your MSI and HSI data\n",
    "\n",
    "# Plot a random sample of the ground truth RGB images and their predictions\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.random.randint(0, len(predictions))\n",
    "    ax.imshow(predictions[idx])\n",
    "    ax.set_title(f'Image #{idx}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
